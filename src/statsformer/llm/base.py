from dataclasses import dataclass, field
import os
import json
import re
import time
import random
from threading import Lock
from typing import Callable
from openai import NOT_GIVEN, BadRequestError, OpenAI
from pydantic import BaseModel

from statsformer.llm.common import LLMConfig
from statsformer.llm.pricing.base import LLMCost, ModelCosts

@dataclass
class LLMOutput:
    """
    Dataclass representing the output of an LLM operation. Handles tracking of
    tokens, costs, retries, and success status.

    Attributes:
        output (str | dict | BaseModel): The output generated by the LLM, which can be a string, dictionary, or a Pydantic BaseModel.
        input_tokens (int): Number of tokens in the input prompt. Defaults to 0.
        output_tokens (int): Number of tokens in the output response. Defaults to 0.
        input_cost (float): Cost associated with the input tokens. Defaults to 0.0.
        output_cost (float): Cost associated with the output tokens. Defaults to 0.0.
        num_retries (int): Number of attempts made to get a successful output. Defaults to 0.
        success (bool): Indicates whether the LLM operation was successful. Defaults to None.
        raw_outputs_per_try (list): List of raw outputs for each attempt.

    Methods:
        total_cost() -> float:
            Returns the total cost (input_cost + output_cost) of the LLM operation.

        blank() -> LLMOutput:
            Returns a blank LLMOutput instance with default values.

        register_attempt(...):
            Registers an attempt to generate output, updating relevant attributes and storing the raw output.

        register_parse_attempt(...):
            Registers the result of a parsing attempt, updating success status,
            i.e., successful if the parsed output is not None.

        register_error():
            Registers an error attempt, increments retry count, and marks success as False.
        
    Example usage:
    ```
    output = LLMOutput.blank()
    for i in range(RETRIES):
        try:
            result = ... call the llm ...
            output.register_attempt(
                output=result,
                input_tokens=...,
                output_tokens=...,
                input_cost=...,
                output_cost=...,
                success=True
            )
            break
        except Exception as e:
            output.register_error()
    if output.success:
        print("LLM call succeeded:", output.output)
    else:
        print(f"LLM call failed after {output.num_retries} retries.")
    ````
    """
    output: str | dict | BaseModel
    cost: LLMCost = field(default_factory=LLMCost)
    num_retries: int = field(default=0)
    success: bool | None = field(default=None)
    raw_outputs_per_try: list = field(default_factory=list)

    def total_cost(self) -> float:
        return self.cost.total_cost()

    @classmethod
    def blank(cls):
        return cls("")

    def register_attempt(
        self,
        output: str | dict | BaseModel,
        input_tokens: int,
        output_tokens: int,
        input_cost: float,
        output_cost: float,
        success: bool = True,
        raw_output: str = None
    ):
        """
        Registers an attempt to generate output, updating relevant attributes
        and storing the raw output.

        This function is used in order to track the cost and token usage
        across retries (e.g., if the LLM is called and gives an invalid JSON,
        this function can be called each time to accumulate the total cost and
        token usage, as well as raw outputs for each try).
        """
        if raw_output is None:
            raw_output = output or ""

        self.output = output
        self.cost += LLMCost(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost
        )
        self.num_retries += 1
        self.success = success
        self.raw_outputs_per_try.append(raw_output)
    
    def register_parse_attempt(
        self,
        parsed: BaseModel | None
    ):
        if parsed is None:
            self.success = False
        else:
            self.output = parsed
            self.success = True

    def register_error(self):
        """
        Registers an error attempt, increments retry count, and marks success
        as False.
        """
        self.num_retries += 1
        self.success = False


def get_system_prompt_json(
    response_model: BaseModel,
    system_prompt: str = "",
):
    instr = "Respond in valid JSON format matching with the following fields:\n\n"
    schema = response_model.model_json_schema()
    required = set(schema.get('required', []))
    for prop in schema['properties']:
        is_required = "required" if prop in required else "optional"
        instr += f"- {prop} ({is_required}): {schema['properties'][prop]}\n"
    instr += "Make sure to follow the JSON format EXACTLY, including data types."

    return f"{system_prompt}\n{instr}" if system_prompt else instr


class LLM:
    """
    LLM provides an interface for interacting with large language models via an API.
    Args:
        - config: LLMConfig object
        - api_key (str, optional): API key for authentication. If not provided,
            will use environment variables.
    Methods:
        call(prompt, system_prompt=None):
            Calls the LLM with a prompt and optional system prompt, returning
                the output.
        call_structured_json(prompt, response_model, system_prompt=None):
            Calls the LLM and parses the output as JSON using the provided
            Pydantic response model. The JSON format of the response is
            enforced via system prompt instructions, and parsed using regex.
        call_structured_pydantic(prompt, response_model=None, system_prompt=None):
            Calls the LLM and parses the output using the provided Pydantic
            response model. This uses the openai client's built-in structured
            response parsing.
    Internal Methods:
        _get_or_create_client():
            Retrieves or creates an API client instance.
        _get_cost(input_tokens, output_tokens):
            Calculates the cost of a request based on token usage.
        _call_api(messages, output, retry_num=0):
            Makes a standard API call and registers the output.
        _call_api_structured_pydantic(messages, response_model, output, retry_num=0):
            Makes an API call expecting structured output and registers the parsed result.
    
    Usage example:
    ```
    llm = LLM(
        config=LLMConfig(
            model_name="gpt-4.1",
            provider="openai",
            temperature=0,
        )
        api_key="your_api_key",
    )
    response = llm.call("What is the capital of France?")
    print(response.output)
    print(f"Cost: ${response.total_cost():.6f}")

    from pydantic import BaseModel
    class MyResponseModel(BaseModel):
        answer: str
        confidence: float
    response = llm.call_structured_json(
        "What is the capital of France?",
        response_model=MyResponseModel
    )
    print(response.output.answer)

    response = llm.call_structured_pydantic(
        "What is the capital of France?",
        response_model=MyResponseModel
    )
    print(response.output.answer)
    ```
    """
    def __init__(
        self,
        config: LLMConfig = None,
        api_key: str = None,
    ):
        self.model_name = config.model_name
        self.base_url = config.get_base_url()
        self.temperature = config.temperature
        if self.temperature is None:
            self.temperature = NOT_GIVEN
        self.max_retries = config.max_retries

        self.client = config.get_client()
        self.model_costs = ModelCosts(config, api_key)
    
    def _call_with_optional_temperature(
        self, messages, response_model: BaseModel | None=None
    ):
        """
        Some reasoning models, like OpenAI o3, do not support setting temperature
        and will error out if provided a temperature.
        """
        try:
            if response_model is None:
                return self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                )
            else:
                return self.client.responses.parse(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                    text_format=response_model
                )
        except BadRequestError as e:
            if 'unsupported' in e.code and e.param == 'temperature':
                self.temperature = NOT_GIVEN
                return self._call_with_optional_temperature(messages)
            raise e

    def _call_api(
        self, messages, output: LLMOutput,
    ):
        """
        INTERNAL: Call the API and handle register the try to an LLMOutput
        object.
        """
        response = self._call_with_optional_temperature(
            messages
        )
        output_text = response.choices[0].message.content
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens

        input_cost, output_cost = self.model_costs.get_cost(input_tokens, output_tokens)
        output.register_attempt(
            output=output_text,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            success=True
        )
    
    def _call_api_structured_pydantic(
        self, messages, response_model: BaseModel,
        output: LLMOutput,
    ):
        """
        INTERNAL: Call the API expecting structured output and handle
        registering the try to an LLMOutput object.
        """
        response = self._call_with_optional_temperature(
            messages, response_model
        )

        output_text = response.choices[0].message.content
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens

        input_cost, output_cost = self.model_costs.get_cost(input_tokens, output_tokens)
        
        output.register_attempt(
            output=response.output_parsed if response.output_parsed else output_text,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            success=response.output_parsed is not None,
            raw_output=output_text
        )

    def call(
        self,
        prompt: str,
        system_prompt: str = None,
        output: LLMOutput | None=None,
    ):
        """
        PUBLIC METHOD: Call the LLM with a prompt and optional system prompt,
        returning the output as an LLMOutput object.
        """
        return self.call_structured_pydantic(
            prompt, response_model=None, system_prompt=system_prompt, output=output
        )

    def call_structured_json(
        self,
        prompt: str,
        response_model: BaseModel,
        system_prompt: str = None,
        output: LLMOutput | None=None,
        output_validator: Callable | None=None,
    ):
        """
        PUBLIC METHOD: Call the LLM and parse the output as JSON using the
        provided Pydantic response model. The JSON format of the response is
        enforced via system prompt instructions, and parsed using regex.
        """
        messages = [
            {"role": "user", "content": prompt}
        ]
        system_prompt = get_system_prompt_json(response_model, system_prompt)
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
        
        if output is None:
            output = LLMOutput.blank()

        retry_num = 0
        while retry_num < self.max_retries + 1:
            prompt_with_retries = prompt
            if retry_num > 0:
                prompt_with_retries =f"[Retry attempt {retry_num}]\n\n{prompt}"
            output = self.call(
                prompt_with_retries, system_prompt, output
            )
            retry_num = output.num_retries
            if not output.success:
                return output
            
            content = output.output
            parsed = None

            # some models don't output JUST the JSON so we need to search for
            # valid JSON first
            jsons = extract_jsons(content)
            errors = []
            for j in jsons:
                try:
                    parsed = response_model.model_validate_json(j)
                    if output_validator is not None:
                        output_validator(parsed)
                    if parsed is not None: # success!
                        break
                except BaseException as e:
                    errors.append(e)
                    parsed = None # in case this failed on output validation
                    continue
            
            output.register_parse_attempt(parsed)
            if parsed is not None:
                break
            print(f"[WARNING] Failed to parse JSON response (attempt {retry_num}/{self.max_retries}).")
            print("Got the following errors trying:")
            print("=" * 20)
            for e in errors:
                print(e)
            print("=" * 20, flush=True)
        return output
        

    def call_structured_pydantic(
        self,
        prompt: str,
        response_model: BaseModel = None,
        system_prompt: str = None,
        output: LLMOutput | None=None,
    ):
        """
        PUBLIC METHOD: Call the LLM and parse the output using the provided
        Pydantic response model. This uses the openai client's built-in structured
        response parsing.
        """
        messages = [
            {"role": "user", "content": prompt}
        ]
        if system_prompt:
            messages.insert(0, {"role": "system", "content": system_prompt})
        
        if output is None:
            output = LLMOutput.blank()

        for retry_num in range(self.max_retries + 1):
            try:
                if response_model:
                    self._call_api_structured_pydantic(
                        messages, response_model, output
                    )
                else:
                    self._call_api(messages, output)
            except Exception as e:
                print(f"[WARNING] LLM call failed with exception: {e}", flush=True)
                error_message = str(e)
                output.register_error()
    
            if output.success or retry_num >= self.max_retries:
                break
            print(f"[WARNING] LLM call failed (attempt {retry_num + 1}/{self.max_retries}): {error_message}", flush=True)
            _backoff_sleep(retry_num)
        return output


def _backoff_sleep(try_i: int):
    time.sleep(min(15, 1.5 * (2 ** try_i) + random.random()))


def extract_jsons(text: str) -> list[str]:
    decoder = json.JSONDecoder()
    results = []
    i = 0
    n = len(text)

    while i < n:
        # Skip until a plausible JSON start
        if text[i] != '{':
            i += 1
            continue

        try:
            _, end = decoder.raw_decode(text, i)
            results.append(text[i:end])
        finally:
            i += 1 

    return results